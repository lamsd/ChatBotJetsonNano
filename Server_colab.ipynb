{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL4FejgV-yPw"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn torch transformers accelerate  pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "model_name = \"gpt2-medium\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "class MessageInput(BaseModel):\n",
        "    messages: list[str]\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "async def generate_text(data: MessageInput):\n",
        "    input_messages = data.messages\n",
        "    input_message_str = \"\\n\".join(input_messages[-20:]) + \"\\nAI:\"\n",
        "\n",
        "    inputs = tokenizer.encode(input_message_str, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(inputs, max_length=inputs.shape[1] + 50, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    last_response = response_text.split(\"AI:\")[-1].strip()\n",
        "\n",
        "    return {\"response\": last_response}\n",
        "\n",
        "@app.post(\"/extract_name\")\n",
        "async def extract_name(data: MessageInput):\n",
        "    input_messages = data.messages\n",
        "    formatted_input_messages = [msg.replace(\"Human: \", \"\") for msg in input_messages if msg.startswith(\"Human: \")]\n",
        "\n",
        "    input_message_str = \"Extract the name from this conversation:\\n\" + \"\\n\".join(formatted_input_messages) + \"\\n\\nName:\"\n",
        "\n",
        "    inputs = tokenizer.encode(input_message_str, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(inputs, max_length=inputs.shape[1] + 10, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    res_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    name = res_text.split(\"Name:\")[-1].strip().lower()\n",
        "\n",
        "    if 'unknown' in name:\n",
        "        return {\"name\": None}\n",
        "\n",
        "    return {\"name\": name}\n",
        "\n",
        "\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "\n",
        "threading.Thread(target=run_server, daemon=True).start()\n"
      ],
      "metadata": {
        "id": "zrWZqxuR-_7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "ngrok.set_auth_token(userdata.get('NGROK_KEY'))\n",
        "\n",
        "\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"API URL:\", public_url)"
      ],
      "metadata": {
        "id": "MBJDaWgl_ofp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}